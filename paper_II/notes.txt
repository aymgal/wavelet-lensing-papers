\paragraph{Some random notes for myself:}
\begin{itemize}
    \item bad sky subtraction can severely bias source light -- possibly lens light as well -- regarding radius and slope estimation. Example: subtracting too much sky led to underestimation of Rsersic (0.8 instead of 1.2) and nsersic (1.1 instead of 1.5). Inaccuracy in estimated sigmabkg (if gaussian noise only) has less impact than inaccuracy in estimating the sky level.
    \item modeling everything at once, i.e. smooth source (Sersic) + smooth potential (SIE) + pixelated potential (PIXELATED): switching off regularization of pixelated potential leads to a best-fit model where the SIE does not play any role and the potential pixels basically fit the input SIE + perturber. SIE parameters are of course completely wrong, and the source radius and Sersic index are vastly impacted $\rightarrow$ can this be explained by a mass-sheet transform?
    \item first scale of Battle-Lemarie wavelet in the regularization helps getting rid of isolated potential pixels.
    \item pixelated potential: the baseline \texttt{minimize} (typically using BFGS method) works well. Proximal gradient descent (PGD) does not lead to great results.
    \item pixelated source: \texttt{minimize} breaks in this setting. PGD is so far the best option \textit{when varying only the source} (fixing lens parameters), since it leads to a noiseless source model. Using \texttt{optax} gradient descent updates lead to a noisier source model, still correctly regularized by starlets, but as uniform additive noise where added to the solution (final reduced chi2 is still 1). UPDATES: it was just either a too high learning rate, or not enough iterations. \texttt{optax} updates generalize well to the situation when optimizing lens parameters simultaneously.
    \item the learning rate (valid for \texttt{optax} optimizers, typically AdaBelief) seems to depend highly on the data being modeled. For instance, a learning rate of $10^{-2}$ worked fine for the mocks, provided a long enough number of iterations. However, for SLACS systems, a learning rate of $10^{-4}$ proves to be the close to the best choice, although it can still vary a bit depending on the system. Nice thing though: changing the resolution of the source (hence number of parameters), does not seem to play any role in the choice of learning rate. \textcolor{red}{from a comment in GLEE documentation: the learning rate should be more or less the expected error bars?}
    \item IDEA for autodiff scheme: side-to-side comparison ``traditional forward mdoeling'' vs. ``autodiff way''
    \item to keep in mind: HMC with NUTS kernel can be waaaay slower than classic kernel!
    \item \url{https://cweniger.github.io/talk-200929-AI4Science/#23}


    - the hessian matrix evaluated at the MAP from the a chi2 loss and {chi2+sparseregul} loss is identical.

\end{itemize}



# TEST2: by releasing smooth parameters during psi_pix fitting -> might have helped a *little* bit to debias posteriors
# TEST3: using l2_norm for final smooth profiles optimisation -> just increase error bars on parameters
# TEST4: final FIM based on chi2 + no regularization (even with pixelated profile) -> get much more meaningful uncertainties on pixpot
# TEST6: remove fully-smooth runs [TEST4 became the baseline]
